# ü§ñ Python AI Framework Comparison

This repository contains a modular, extensible framework for testing and comparing Large Language Models (LLMs) such as `llama3`, `mistral`, and more. It allows for standardized prompt evaluation, automated scoring, database logging, and visual reporting.

> Built to help engineers and researchers assess speed, quality, and consistency of AI models with minimal setup.

---

## üìå Table of Contents

- [Features](#features)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Usage Example](#usage-example)
- [Scoring Metrics](#scoring-metrics)
- [Test Categories](#test-categories)
- [Sample Report](#sample-report)
- [Dependencies](#dependencies)
- [Roadmap](#roadmap)
- [Author](#author)

---

## üöÄ Features

- üîå Plugin-style architecture for model providers (Ollama, OpenAI, etc.)
- üß™ Runs categorized prompt tests across any model
- üß† Intelligent response scoring:
  - Length analysis
  - Factual confidence
  - Code quality
  - Creativity detection
- üóÇÔ∏è Built-in test cases across math, reasoning, coding, factual, and creative categories
- üìä HTML + CSV reports with comparative insights
- üóÉÔ∏è SQLite storage of all results for future querying

---

## üóÇÔ∏è Project Structure

```
python-ai-frameworkComparison/
‚îú‚îÄ‚îÄ llm_test_framework.py         # Main testing framework
‚îú‚îÄ‚îÄ export_my_results.py          # CSV/HTML export script
‚îú‚îÄ‚îÄ sampleTest.py                 # Example test runner
‚îú‚îÄ‚îÄ test_runner.py                # CLI execution
‚îú‚îÄ‚îÄ testingLLM.py                 # Integration runner
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ test_results/
    ‚îú‚îÄ‚îÄ llm_test_results.db
    ‚îú‚îÄ‚îÄ model_comparison_results.json
    ‚îú‚îÄ‚îÄ llm_test_results_*.csv
    ‚îî‚îÄ‚îÄ llm_test_summary_*.html
```

---

## üõ†Ô∏è Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/String-Gaurav/python-ai-frameworkComparison.git
cd python-ai-frameworkComparison
```

### 2. Set Up Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the Framework

```bash
python test_runner.py
```

---

## üí° Usage Example

```python
from llm_test_framework import LLMTestFramework

framework = LLMTestFramework()
framework.add_ollama_model("llama3.2:latest")

# Run math category test
results = framework.run_test_suite(categories=["math"])

# Generate reports
framework.generate_report(results)
framework.export_results_csv()
```

---

## üìä Scoring Metrics

Each model response is scored based on:

| Metric           | Applied To     | Description                               |
|------------------|----------------|-------------------------------------------|
| `length_score`   | All types      | Penalizes too short/long answers          |
| `code_quality`   | Coding         | Keyword and syntax indicator check        |
| `confidence`     | Factual        | Favors confident vs uncertain phrases     |
| `creativity`     | Creative       | Looks for storytelling and variation      |
| `math_content`   | Math           | Checks for numeric/symbolic content       |
| `overall_quality`| All types      | Combined result of above per type         |

---

## üß™ Test Categories

Built-in test suite covers:

- üßÆ `math` ‚Äì numerical logic
- üíª `code` ‚Äì Python generation
- üí° `factual` ‚Äì knowledge prompts
- üß† `reasoning` ‚Äì logical deduction
- ‚úçÔ∏è `creative` ‚Äì storytelling, poetry

Each test case includes:
- ID, prompt, type, expected output, category, difficulty, and tags

---

## üìà Sample Report

Example output:

| Model           | Response Time | Quality Score | Tests Run |
|-----------------|---------------|----------------|-----------|
| llama3.2:latest | 4.04s         | 0.83           | 14        |
| mistral:7b      | 7.10s         | 0.83           | 14        |

‚úÖ **Key Insight**: llama3.2:latest maintained equal quality with 76% faster response time vs mistral:7b.

üìÅ Reports are saved under `/test_results/` in `.csv`, `.json`, and `.html` formats.

---

## üì¶ Dependencies

Install all required packages using:

```bash
pip install -r requirements.txt
```

Content of `requirements.txt`:

```txt
ollama
pandas
jinja2
beautifulsoup4
```

---

## üß≠ Roadmap

- [x] SQLite-backed persistent storage
- [x] Visual HTML report with comparison
- [ ] OpenAI & Claude support
- [ ] Parallel test execution (multithreaded)
- [ ] CLI test filtering (by category, model)
- [ ] Graphs & charts in reports (Plotly or Chart.js)

---

## üë®‚Äçüíª Author

**Gaurav Singh**

- üåç Portfolio: [gauravsingh-info.netlify.app](https://gauravsingh-info.netlify.app)
- üíº LinkedIn: [linkedin.com/in/gaurav-singh27](https://www.linkedin.com/in/gaurav-singh27/)
- üìß Email: [gaurav10690@gmail.com](mailto:gaurav10690@gmail.com)

> ‚ú® If you find this project useful, please consider giving it a ‚≠ê on GitHub!

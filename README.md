# ğŸ¤– Python AI Framework Comparison

This repository contains a modular, extensible framework for testing and comparing Large Language Models (LLMs) such as `llama3`, `mistral`, and more. It allows for standardized prompt evaluation, automated scoring, database logging, and visual reporting.

> Built to help engineers and researchers assess speed, quality, and consistency of AI models with minimal setup.

---

## ğŸ“Œ Table of Contents

## Table of Contents

- [Features](#features)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Usage Example](#usage-example)
- [Scoring Metrics](#scoring-metrics)
- [Test Categories](#test-categories)
- [Sample Report](#sample-report)
- [Dependencies](#dependencies)
- [Roadmap](#roadmap)
- [Author](#author)


---

## ğŸš€ Features

- ğŸ”Œ Plugin-style architecture for model providers (Ollama, OpenAI, etc.)
- ğŸ§ª Runs categorized prompt tests across any model
- ğŸ§  Intelligent response scoring:
  - Length analysis
  - Factual confidence
  - Code quality
  - Creativity detection
- ğŸ—‚ï¸ Built-in test cases across math, reasoning, coding, factual, and creative categories
- ğŸ“Š HTML + CSV reports with comparative insights
- ğŸ—ƒï¸ SQLite storage of all results for future querying

---

## ğŸ—‚ï¸ Project Structure

```
python-ai-frameworkComparison/
â”œâ”€â”€ llm_test_framework.py         # Main testing framework
â”œâ”€â”€ export_my_results.py          # CSV/HTML export script
â”œâ”€â”€ sampleTest.py                 # Example test runner
â”œâ”€â”€ test_runner.py                # CLI execution
â”œâ”€â”€ testingLLM.py                 # Integration runner
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ test_results/
    â”œâ”€â”€ llm_test_results.db
    â”œâ”€â”€ model_comparison_results.json
    â”œâ”€â”€ llm_test_results_*.csv
    â””â”€â”€ llm_test_summary_*.html
```

---

## ğŸ› ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/String-Gaurav/python-ai-frameworkComparison.git
cd python-ai-frameworkComparison
```

### 2. Set Up Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the Framework

```bash
python test_runner.py
```

---

## ğŸ’¡ Usage Example

```python
from llm_test_framework import LLMTestFramework

framework = LLMTestFramework()
framework.add_ollama_model("llama3.2:latest")

# Run math category test
results = framework.run_test_suite(categories=["math"])

# Generate reports
framework.generate_report(results)
framework.export_results_csv()
```

---

## ğŸ“Š Scoring Metrics

Each model response is scored based on:

| Metric           | Applied To     | Description                               |
|------------------|----------------|-------------------------------------------|
| `length_score`   | All types      | Penalizes too short/long answers          |
| `code_quality`   | Coding         | Keyword and syntax indicator check        |
| `confidence`     | Factual        | Favors confident vs uncertain phrases     |
| `creativity`     | Creative       | Looks for storytelling and variation      |
| `math_content`   | Math           | Checks for numeric/symbolic content       |
| `overall_quality`| All types      | Combined result of above per type         |

---

## ğŸ§ª Test Categories

Built-in test suite covers:

- ğŸ§® `math` â€“ numerical logic
- ğŸ’» `code` â€“ Python generation
- ğŸ’¡ `factual` â€“ knowledge prompts
- ğŸ§  `reasoning` â€“ logical deduction
- âœï¸ `creative` â€“ storytelling, poetry

Each test case includes:
- ID, prompt, type, expected output, category, difficulty, and tags

---

## ğŸ“ˆ Sample Report

Example output:

| Model           | Response Time | Quality Score | Tests Run |
|-----------------|---------------|----------------|-----------|
| llama3.2:latest | 4.04s         | 0.83           | 14        |
| mistral:7b      | 7.10s         | 0.83           | 14        |

âœ… **Key Insight**: llama3.2:latest maintained equal quality with 76% faster response time vs mistral:7b.

ğŸ“ Reports are saved under `/test_results/` in `.csv`, `.json`, and `.html` formats.

<img width="850" height="923" alt="image" src="https://github.com/user-attachments/assets/35c832d1-3a59-4dc7-ace1-e17bdbb4c3ce" />


---

## ğŸ“¦ Dependencies

Install all required packages using:

```bash
pip install -r requirements.txt
```

Content of `requirements.txt`:

```txt
ollama
pandas
jinja2
beautifulsoup4
```

---

## ğŸ§­ Roadmap

- [x] SQLite-backed persistent storage
- [x] Visual HTML report with comparison
- [ ] OpenAI & Claude support
- [ ] Parallel test execution (multithreaded)
- [ ] CLI test filtering (by category, model)
- [ ] Graphs & charts in reports (Plotly or Chart.js)

---

## ğŸ‘¨â€ğŸ’» Author

**Gaurav Singh**

- ğŸŒ Portfolio: [gauravsingh-info.netlify.app](https://gauravsingh-info.netlify.app)
- ğŸ’¼ LinkedIn: [linkedin.com/in/gaurav-singh27](https://www.linkedin.com/in/gaurav-singh27/)
- ğŸ“§ Email: [gaurav10690@gmail.com](mailto:gaurav10690@gmail.com)

> âœ¨ If you find this project useful, please consider giving it a â­ on GitHub!
